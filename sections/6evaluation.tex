\section{Evaluation}
\label{sec:evaluation}
The extraction has been conducted as a proof-of-concept on four major WLE:
The English, French, Russian and German \wik.
The datasets combined contain more than 100 million facts\footnote{SPARQL: \texttt{SELECT COUNT(*) WHERE {?s ?p ?o}}} about 5 million lexical words\footnote{SPARQL: \texttt{SELECT COUNT(?s) WHERE {?s a lemon:LexicalEntry}}} .
The data is available as N-Triples dumps\footnote{\url{http://downloads.dbpedia.org/wiktionary}}, Linked Data\footnote{for example \url{http://wiktionary.dbpedia.org/resource/dog}}, via the \emph{Virtuoso Faceted Browser}\footnote{\url{http://wiktionary.dbpedia.org/fct}} or a SPARQL endpoint\footnote{\url{http://wiktionary.dbpedia.org/sparql}}.

\subsection{Example Data}


\subsection{Quantity Measurement}
We used some simple counting queries to measure the \textit{dimensions} of the RDF data. This includes number of it entries in the dictionary, or when seen as a graph, the number of edges and vertices or the number of distinct used predicates. 

\begin{threeparttable} 
\begin{tabular}{|l|r|r|r|r|r|r|r|r|}
\hline \emph{language} & \emph{\#words} & \emph{\#triples} & \emph{\#resources}  & \emph{\#predicates} & \emph{\#senses} \\ 
\hline \hline \textit{en} & 2,903,933 & 71,230,704 & 33,428,598 & 26 & 966,673 \\ 
\hline \textit{fr} & 2,093,017 & 32,530,177 & 20,241,644 & 21 & 793,640 \\ 
\hline \textit{de} & 204,045 & 6,677,192 & 3,448,052 & 23 & 170,762 \\ 
\hline 
\end{tabular}
\caption{Statistical quantity comparison of three \wik extraction result datasets.}
\end{threeparttable}

The statistics show, that the extraction produces a vast amount of data with broad coverage, thus resulting in the largest lexical linked data resource. 


\subsection{Quality Measurement}
The measurement of data quality is a difficult topic, as there is no gold standard --- no optimum to compare with. 
One could compare with competing approaches, but what if you succeed them? 
When your scope is too different? 
It is necessary to use absolute measures, either automatically calculated (which can be misleading) or by human rating (which requires substantial effort).

\begin{threeparttable} 
\begin{tabular}{|l|c|c|c|c|}
\hline language & \emph{t/w}\tnote{a}\hspace{0.15cm} & \emph{\#wws}\tnote{b}\hspace{0.15cm} & \emph{s/wws}\tnote{c}\hspace{0.15cm} & \emph{t/l}\tnote{d}\hspace{0.15cm} \\
\hline \hline \textit{en} & 24.52 & 708,644 & 1.36 & \\
\hline \textit{fr} & 15.54 & 628,299 & 1.26 & \\ 
\hline \textit{de} & 32.72 & 116,622 & 1.46 & \\ 
\hline 
\end{tabular} 
\caption{Statistical quality comparison of three \wik extraction result datasets.}
\begin{tablenotes}\footnotesize 
\item[a] \textit{Triples per word.} The simplest measure of information density.
\item[b] \textit{Words with senses.} The number of words, that have at least one sense  extracted. 
An indicator for the ratio of pages for which valuable information could be extracted (but consider stub pages, that are actually empty)
\item[c] \textit{Senses per word with sense.} 
\item[d] \textit{Triples per line.} The number of triples divided by the number of line breaks in the page source (plus one). 
Averaged across all pages.
\end{tablenotes}
\end{threeparttable}

All presented index numbers are chosen at will, to give some idea about the coverage of the RDF data. 
But none is able to indicate the quality of the extraction --- the completeness of the configuration --- on a scale from zero to one. 
The numbers depend on the quality of the source data (which can not simply be assessed) and not necessarily are normed to one.
It can be argued, that the last measure, triples per line, may the one most robust against source dependency (the tendency of the measure to vary with the quality of the source, which is desired to be low). 
It can be argued that this value should (for a perfect extraction configuration) be close to one, because each line should contain some information (which results in a triple). 
There are empty lines and lines that produce multiple triples --- but these lines are rarer and should eliminate each other (also empty lines could be disregarded easily). 
Therefore any value considerably lower than one, indicates that there are many uninterpreted lines. 
But again, these measures might be misleading as they are influenced by many unknown factors. 
Do not use them to compare two languages editions of Wiktionary or a new configuration against one for a different language which is considered \textit{good}.
A safe way to use them is when comparing two versions of a configuration files with each other.

Both measurement types can be conducted with the \texttt{statistics} tool, which is part of the source code. 
It operates on the N-Triples dump of one language. 

A reliable data quality assessment is only possible by human rating of randomly sampled entries. 
The procedure would include the random selection of a sufficiently large subset of extracted data (e.g. 200 words within one WLE), then these are assigned to a group of semi-experts to validate them against a check list of common errors (e.g. missing data, incorrect data, wrong datatype, etc.).

\subsection{Maintenance Experience}
One of the claims of this thesis is the easy maintenance of the configuration and it is crucial that non-professionals can edit them. 
To evaluate this trait we let a colleague with no special foreknowledge build the configuration for the Russian configuration. 
He took a few days to get familiar with the topic (and doing his normal work as well), and then was able to create a quite good configuration himself. 
To fully evaluate this claim we need to wait until the project is picked up in the wild and interview adopters, unfortunately this is not possible within the temporal constraints of this thesis.

\subsection{Limitations}


\newpage
