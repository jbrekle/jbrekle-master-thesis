\section{Conclusion}
\subsection{Vision}
\textit{Making unstructured sources machine-readable creates feedback loops.}
The argument that extracting structured data from an open data source and making it freely available in turn encourages users of the extracted data to contribute to the unstructured source, seems reasonable. It is firstly not easily possible to continuously apply changes to the automatically extracted data when the source and the configuration changes arbitrarily. One could imagine a kind of a patch queue, but also how hard it is to maintain it. On the other hand, for humans it is much easier to curate a wiki. The co-evolution of ontologies is still an open research problem.
Back to the claim: Users of the RDF version of Wiktionary could be NLP researchers or companies. Some of them have considerable human resources, that maintain internal databases of linguistic knowledge. And some are willing to publish their data and integrate it with existing sources; but the clear incentive is to \textit{get the data back again} --- enriched. With an mature extraction infrastructure (that can be deployed autonomously by adopters or centrally by us at the same time), it becomes reasonable for such third parties to contribute.
This increase in participation besides improving the source, also illustrates the advantages of machine readable data to common Wiktionarians. Which are in turn more motivated to cooperate. Such considerations are fundamental; without community interaction and mutual benefits, extraction from community edited sources becomes dull scraping. There is an implicit requirement for a social effort to successfully transfer the desired properties of the source to the extracted data.
Such a positive effect from DBpedia supported the current \textit{Wikidata}\footnote{\url{http://meta.wikimedia.org/wiki/Wikidata}} project.

\subsection{Suggested changes to Wiktionary}
Although it's hard to persuade the community of far-reaching changes, we want to conclude how \wik can increase its data quality and enable better extraction.
\begin{compactitem}
\item \textbf{Homogenize Entry Layout across all WLE's.}
\item \textbf{Use anchors to markup senses:}
	This implies creating URIs for senses.
	These can then be used to be more specific when referencing a \textit{word} from another article. 
	This would greatly benefit the evaluation of automatic anchoring approaches like in \cite{meyer_2011b}.
\item \textbf{Word forms:}
	The notion of word forms (e.g. declensions or conjugations) is not consistent across articles.
	They are hard to extract and often not given.
\end{compactitem}

\subsection{Discussion}
Our main contributions are (1) an extremely flexible extraction from \wik, with simple adaption to new Wiktionaries and changes via a declarative configuration. 
By doing so, we are (2) provisioning a linguistic knowledge base with unprecedented detail and coverage. 
The DBpedia project provides (3) a mature, reusable infrastructure including a public Linked Data service and SPARQL endpoint. All resources related to our \wik extraction, such as source-code, extraction results, pointers to applications etc. are available from our project page\footnote{\url{http://wiktionary.dbpedia.org}}.
As a result, we hope it will evolve into a central resource and interlinking hub on the currently emerging Web of Linguistic Data.

\subsection{Next Steps}
\textbf{Wiktionary Live:}
Users constantly revise articles.
Hence, data can quickly become outdated, and articles need to be re-extracted. DBpedia-Live enables such a continuous synchronization between DBpedia and Wikipedia.
The WikiMedia foundation kindly provided us access to their update stream, the Wikipedia OAI-PMH\footnote{Open Archives Initiative Protocol for Metadata Harvesting,\\
cf. \url{http://www.mediawiki.org/wiki/Extension:OAIRepository}} live feed.
The approach is equally applicable to \wik. 
The \wik Live extraction will enable users for the first time ever to query \wik like a database in real-time and receive up-to-date data in a machine-readable format.
This will strengthen \wik as a central resource and allow it to extend its coverage and quality even more.\\
\textbf{Wiki based UI for the WLE configurations:}
To enable the crowd-sourcing of the extractor configuration, an intuitive web interface is desirable. Analogue to the mappings wiki\footnote{\url{http://mappings.dbpedia.org/}} of DBpedia, a wiki could help to hide the technical details of the configuration even more. 
Therefore a JavaScript based WYSIWYG XML editor seems useful. 
There are various implementations, which can be easily adapted.\\
\textbf{Linking:}
Finally, an alignment with existing linguistic resources like WordNet and general ontologies like YAGO or DBpedia is essential. That way \wik will allow for the interoperability across a multilingual semantic web.\\
\textbf{Quality Assessment:}
Analogously to DBpedia and 

\subsection{Open Research Questions}
\subsubsection{Publishing Lexica as Linked Data}
The need to publish lexical resources as linked data has been recognized recently~\cite{NuzzoleseEtAl_KCAP2011}.
Although principles for publishing RDF as Linked Data are already well established~\cite{auer-swj-2010,linkeddata-book}, the choice of identifiers and first-class objects is crucial for any linking approach.
A number of questions need to be clarified, such as which entities in the lexicon can be linked to others. 
Obvious candidates are entries, senses, synsets, lexical forms, languages, ontology instances and classes, but different levels of granularity have to be considered and a standard linking relation such as \texttt{owl:sameAs} will not be sufficient. 
Linking across data sources is at the heart of linked data. 
An open question is how lexical resources with differing schemata can be linked and how are linguistic entities to be linked with ontological ones. 
There is most certainly an impedance mismatch to bridge.

%The success of DBpedia~\cite{dbpedia_jws_09} as a ``crystallization point for the Web of Data'' is predicated on the stable identifiers provided by Wikipedia and are an obvious prerequisite for any data authority.
The success of DBpedia as a ``crystallization point for the Web of Data'' is predicated on the stable identifiers provided by Wikipedia and are an obvious prerequisite for any data authority.
Our approach has the potential to drive this process by providing best practices and live showcases and data in the same way DBpedia has provided it for the LOD cloud. 
Especially, our work has to be seen in the context of the recently published Linguistic Linked Data Cloud\cite{CHIARCOS12.912} and the community effort around the Open Linguistics Working Group (OWLG)\footnote{\url{http://linguistics.okfn.org}} and NIF~\cite{hellmann-2012-ekaw}.
Our Wiktionary conversion project provides valuable data dumps and linked data services to further fuel development in this area.  


\subsubsection{Algorithms and methods to bootstrap and maintain a Lexical Linked Data Web}

State-of-the-art approaches for interlinking instances in RDF knowledge bases are mainly build upon similarity metrics~\cite{NGAU11,conf/semweb/VolzBGK09} to find duplicates in the data, linkable via \texttt{owl:sameAs}.
Such approaches are not directly applicable to lexical data.
Existing linking properties either carry strong formal implications (e.g. \texttt{owl:sameAs}) or do not carry sufficient domain-specific information for modelling semantic relations between lexical knowledge bases.

\newpage
